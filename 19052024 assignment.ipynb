{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972bb43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Step 1: Navigate to the website\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in the search field\n",
    "search_field = driver.find_element(By.ID, \"qsb-keyword-sugg\")\n",
    "search_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Step 3: Click the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[@class='btn']\")\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"list\")))\n",
    "\n",
    "# Step 4: Apply location filter \"Delhi/NCR\"\n",
    "location_filter = driver.find_element(By.XPATH, \"//span[text()='Delhi / NCR']\")\n",
    "location_filter.click()\n",
    "\n",
    "# Wait for the filter to apply\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 5: Apply salary filter \"3-6 Lakhs\"\n",
    "salary_filter = driver.find_element(By.XPATH, \"//span[text()='3-6 Lakhs']\")\n",
    "salary_filter.click()\n",
    "\n",
    "# Wait for the filter to apply\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 6: Scrape the first 10 job results\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "# Scroll down to load more jobs\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "jobs = soup.find_all('article', class_='jobTuple bgWhite br4 mb-8', limit=10)\n",
    "\n",
    "for job in jobs:\n",
    "    job_title = job.find('a', class_='title fw500 ellipsis').text.strip()\n",
    "    job_location = job.find('li', class_='fleft grey-text br2 placeHolderLi location').text.strip()\n",
    "    company_name = job.find('a', class_='subTitle ellipsis fleft').text.strip()\n",
    "    experience = job.find('li', class_='fleft grey-text br2 placeHolderLi experience').text.strip()\n",
    "\n",
    "    job_titles.append(job_title)\n",
    "    job_locations.append(job_location)\n",
    "    company_names.append(company_name)\n",
    "    experience_required.append(experience)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Step 7: Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Job Title': job_titles,\n",
    "    'Job Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('naukri_data_scientist_jobs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Step 1: Navigate to the website\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in the search field and \"Bangalore\" in the location field\n",
    "job_title_field = driver.find_element(By.ID, \"id_q\")\n",
    "job_title_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "location_field = driver.find_element(By.ID, \"id_loc\")\n",
    "location_field.send_keys(\"Bangalore\")\n",
    "\n",
    "# Step 3: Click the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[contains(text(),'Search Jobs')]\")\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"job_list\")))\n",
    "\n",
    "# Step 4: Scrape the data for the first 10 job results\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "# Scroll down to load more jobs\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "jobs = soup.find_all('li', class_='result-display__profile', limit=10)\n",
    "\n",
    "for job in jobs:\n",
    "    job_title = job.find('h2').text.strip()\n",
    "    job_location = job.find('div', class_='result-display__profile__company__loc').text.strip()\n",
    "    company_name = job.find('div', class_='result-display__profile__company').text.strip()\n",
    "    experience = job.find('div', class_='result-display__profile__experience').text.strip()\n",
    "\n",
    "    job_titles.append(job_title)\n",
    "    job_locations.append(job_location)\n",
    "    company_names.append(company_name)\n",
    "    experience_required.append(experience)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Job Title': job_titles,\n",
    "    'Job Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('shine_data_scientist_jobs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aab7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the Flipkart page for iPhone 11 reviews\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "driver.get(url)\n",
    "\n",
    "# Function to extract reviews from the current page\n",
    "def extract_reviews(soup):\n",
    "    reviews = []\n",
    "    for review in soup.find_all('div', {'class': '_1AtVbE'}):\n",
    "        rating = review.find('div', {'class': '_3LWZlK _1BLPMq'})\n",
    "        review_summary = review.find('p', {'class': '_2-N8zT'})\n",
    "        full_review = review.find('div', {'class': 't-ZTKy'})\n",
    "        if rating and review_summary and full_review:\n",
    "            reviews.append({\n",
    "                'Rating': rating.text,\n",
    "                'Review Summary': review_summary.text,\n",
    "                'Full Review': full_review.div.div.text\n",
    "            })\n",
    "    return reviews\n",
    "\n",
    "# Initialize a list to hold all reviews\n",
    "all_reviews = []\n",
    "\n",
    "# Number of reviews we want to scrape\n",
    "num_reviews_to_scrape = 100\n",
    "\n",
    "# Keep track of the number of reviews scraped\n",
    "reviews_scraped = 0\n",
    "\n",
    "# Loop through the pages and extract reviews\n",
    "while reviews_scraped < num_reviews_to_scrape:\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extract reviews from the current page\n",
    "    reviews = extract_reviews(soup)\n",
    "    \n",
    "    # Add the reviews to the list\n",
    "    all_reviews.extend(reviews)\n",
    "    \n",
    "    # Update the count of reviews scraped\n",
    "    reviews_scraped = len(all_reviews)\n",
    "    \n",
    "    # Break if we've scraped enough reviews\n",
    "    if reviews_scraped >= num_reviews_to_scrape:\n",
    "        break\n",
    "    \n",
    "    # Find and click the \"Next\" button to go to the next page of reviews\n",
    "    next_button = driver.find_element(By.XPATH, '//a[@class=\"_1LKTO3\"]')\n",
    "    if next_button:\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Wait for the next page to load\n",
    "    else:\n",
    "        break  # No more pages to load\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Trim the reviews to the desired number\n",
    "all_reviews = all_reviews[:num_reviews_to_scrape]\n",
    "\n",
    "# Create a DataFrame from the reviews\n",
    "df = pd.DataFrame(all_reviews)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('flipkart_iphone11_reviews.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68677bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Flipkart's website\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(2)  # Allow the page to load\n",
    "\n",
    "# Close the login pop-up if it appears\n",
    "try:\n",
    "    close_button = driver.find_element(By.XPATH, '//button[contains(text(),\"✕\")]')\n",
    "    close_button.click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Enter \"sneakers\" in the search field\n",
    "search_box = driver.find_element(By.NAME, \"q\")\n",
    "search_box.send_keys(\"sneakers\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "time.sleep(2)  # Allow search results to load\n",
    "\n",
    "# Function to extract sneaker details from the current page\n",
    "def extract_sneakers(soup):\n",
    "    sneakers = []\n",
    "    for item in soup.find_all('div', {'class': '_1AtVbE'}):\n",
    "        brand = item.find('div', {'class': '_2WkVRV'})\n",
    "        product_description = item.find('a', {'class': 'IRpwTa'})\n",
    "        price = item.find('div', {'class': '_30jeq3'})\n",
    "        if brand and product_description and price:\n",
    "            sneakers.append({\n",
    "                'Brand': brand.text,\n",
    "                'Product Description': product_description.text,\n",
    "                'Price': price.text\n",
    "            })\n",
    "    return sneakers\n",
    "\n",
    "# Initialize a list to hold all sneakers\n",
    "all_sneakers = []\n",
    "\n",
    "# Number of sneakers we want to scrape\n",
    "num_sneakers_to_scrape = 100\n",
    "\n",
    "# Keep track of the number of sneakers scraped\n",
    "sneakers_scraped = 0\n",
    "\n",
    "# Loop through the pages and extract sneakers\n",
    "while sneakers_scraped < num_sneakers_to_scrape:\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extract sneakers from the current page\n",
    "    sneakers = extract_sneakers(soup)\n",
    "    \n",
    "    # Add the sneakers to the list\n",
    "    all_sneakers.extend(sneakers)\n",
    "    \n",
    "    # Update the count of sneakers scraped\n",
    "    sneakers_scraped = len(all_sneakers)\n",
    "    \n",
    "    # Break if we've scraped enough sneakers\n",
    "    if sneakers_scraped >= num_sneakers_to_scrape:\n",
    "        break\n",
    "    \n",
    "    # Find and click the \"Next\" button to go to the next page of results\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_1LKTO3\"]')\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Wait for the next page to load\n",
    "    except:\n",
    "        break  # No more pages to load or unable to find the next button\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Trim the sneakers to the desired number\n",
    "all_sneakers = all_sneakers[:num_sneakers_to_scrape]\n",
    "\n",
    "# Create a DataFrame from the sneakers\n",
    "df = pd.DataFrame(all_sneakers)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('flipkart_sneakers.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65498a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Amazon's website\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "time.sleep(2)  # Allow the page to load\n",
    "\n",
    "# Enter \"Laptop\" in the search field\n",
    "search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_box.send_keys(\"Laptop\")\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "time.sleep(2)  # Allow search results to load\n",
    "\n",
    "# Apply CPU Type filter to \"Intel Core i7\"\n",
    "# Wait for the filter section to load and click the \"Intel Core i7\" filter\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//li[@aria-label=\"Intel Core i7\"]//i'))\n",
    "    ).click()\n",
    "    time.sleep(2)  # Allow filter results to load\n",
    "except Exception as e:\n",
    "    print(f\"Error applying filter: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Function to extract laptop details from the current page\n",
    "def extract_laptops(soup):\n",
    "    laptops = []\n",
    "    for item in soup.find_all('div', {'data-component-type': 's-search-result'}):\n",
    "        title = item.h2.text\n",
    "        rating = item.find('span', {'class': 'a-icon-alt'})\n",
    "        price = item.find('span', {'class': 'a-price-whole'})\n",
    "        if title and rating and price:\n",
    "            laptops.append({\n",
    "                'Title': title.strip(),\n",
    "                'Ratings': rating.text.strip(),\n",
    "                'Price': price.text.strip()\n",
    "            })\n",
    "    return laptops\n",
    "\n",
    "# Initialize a list to hold all laptops\n",
    "all_laptops = []\n",
    "\n",
    "# Number of laptops we want to scrape\n",
    "num_laptops_to_scrape = 10\n",
    "\n",
    "# Keep track of the number of laptops scraped\n",
    "laptops_scraped = 0\n",
    "\n",
    "# Loop through the pages and extract laptops\n",
    "while laptops_scraped < num_laptops_to_scrape:\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extract laptops from the current page\n",
    "    laptops = extract_laptops(soup)\n",
    "    \n",
    "    # Add the laptops to the list\n",
    "    all_laptops.extend(laptops)\n",
    "    \n",
    "    # Update the count of laptops scraped\n",
    "    laptops_scraped = len(all_laptops)\n",
    "    \n",
    "    # Break if we've scraped enough laptops\n",
    "    if laptops_scraped >= num_laptops_to_scrape:\n",
    "        break\n",
    "    \n",
    "    # Find and click the \"Next\" button to go to the next page of results\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//li[@class=\"a-last\"]/a')\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Wait for the next page to load\n",
    "    except:\n",
    "        break  # No more pages to load or unable to find the next button\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Trim the laptops to the desired number\n",
    "all_laptops = all_laptops[:num_laptops_to_scrape]\n",
    "\n",
    "# Create a DataFrame from the laptops\n",
    "df = pd.DataFrame(all_laptops)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('amazon_laptops.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e22b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open AZQuotes website\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "time.sleep(2)  # Allow the page to load\n",
    "\n",
    "# Click on \"Top Quotes\"\n",
    "top_quotes_link = driver.find_element(By.LINK_TEXT, \"Top Quotes\")\n",
    "top_quotes_link.click()\n",
    "time.sleep(2)  # Allow the top quotes page to load\n",
    "\n",
    "# Function to extract quotes from the current page\n",
    "def extract_quotes(soup):\n",
    "    quotes_data = []\n",
    "    quotes = soup.find_all('div', class_='title')\n",
    "    authors = soup.find_all('div', class_='author')\n",
    "    types = soup.find_all('div', class_='tags')\n",
    "    \n",
    "    for quote, author, quote_type in zip(quotes, authors, types):\n",
    "        quotes_data.append({\n",
    "            'Quote': quote.text.strip(),\n",
    "            'Author': author.text.strip(),\n",
    "            'Type Of Quotes': quote_type.text.strip().replace('Tags: ', '')\n",
    "        })\n",
    "    \n",
    "    return quotes_data\n",
    "\n",
    "# Initialize a list to hold all quotes\n",
    "all_quotes = []\n",
    "\n",
    "# Number of quotes we want to scrape\n",
    "num_quotes_to_scrape = 1000\n",
    "\n",
    "# Keep track of the number of quotes scraped\n",
    "quotes_scraped = 0\n",
    "\n",
    "# Loop through the pages and extract quotes\n",
    "while quotes_scraped < num_quotes_to_scrape:\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Extract quotes from the current page\n",
    "    quotes = extract_quotes(soup)\n",
    "    \n",
    "    # Add the quotes to the list\n",
    "    all_quotes.extend(quotes)\n",
    "    \n",
    "    # Update the count of quotes scraped\n",
    "    quotes_scraped = len(all_quotes)\n",
    "    \n",
    "    # Break if we've scraped enough quotes\n",
    "    if quotes_scraped >= num_quotes_to_scrape:\n",
    "        break\n",
    "    \n",
    "    # Find and click the \"Next\" button to go to the next page of results\n",
    "    try:\n",
    "        next_button = driver.find_element(By.LINK_TEXT, 'Next')\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Wait for the next page to load\n",
    "    except:\n",
    "        break  # No more pages to load or unable to find the next button\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Trim the quotes to the desired number\n",
    "all_quotes = all_quotes[:num_quotes_to_scrape]\n",
    "\n",
    "# Create a DataFrame from the quotes\n",
    "df = pd.DataFrame(all_quotes)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('top_1000_quotes.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85157815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = \"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the webpage content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the data\n",
    "table = soup.find('table')\n",
    "\n",
    "# Initialize lists to hold the data\n",
    "names = []\n",
    "born_dead = []\n",
    "terms_of_office = []\n",
    "remarks = []\n",
    "\n",
    "# Loop through the rows of the table (skip the header)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) >= 4:  # Ensure there are enough columns\n",
    "        names.append(cells[0].get_text(strip=True))\n",
    "        born_dead.append(cells[1].get_text(strip=True))\n",
    "        terms_of_office.append(cells[2].get_text(strip=True))\n",
    "        remarks.append(cells[3].get_text(strip=True))\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': terms_of_office,\n",
    "    'Remarks': remarks\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('former_prime_ministers_of_india.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a311cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# Set up WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "driver.get('https://www.motor1.com/')\n",
    "\n",
    "# Step 2: Type in the search bar '50 most expensive cars'\n",
    "search_bar = driver.find_element(By.NAME, 'query')\n",
    "search_bar.send_keys('50 most expensive cars')\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(3)  # Wait for the results to load\n",
    "\n",
    "# Step 3: Click on '50 most expensive cars in the world..'\n",
    "link = driver.find_element(By.PARTIAL_LINK_TEXT, '50 most expensive cars in the world')\n",
    "link.click()\n",
    "\n",
    "time.sleep(5)  # Wait for the page to load completely\n",
    "\n",
    "# Step 4: Scrape the mentioned data and make the dataframe\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Find car names and prices\n",
    "cars = soup.find_all('h3')\n",
    "car_names = []\n",
    "car_prices = []\n",
    "\n",
    "for car in cars:\n",
    "    car_info = car.get_text().strip()\n",
    "    if ' - ' in car_info:\n",
    "        name, price = car_info.split(' - ')\n",
    "        car_names.append(name.strip())\n",
    "        car_prices.append(price.strip())\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Car Name': car_names,\n",
    "    'Price': car_prices\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('most_expensive_cars.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
