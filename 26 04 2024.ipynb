{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebcfd11",
   "metadata": {},
   "source": [
    "Write a python program to display IMDB’s Top rated 100 Indian movies’ data\n",
    "https://www.imdb.com/list/ls056092300/ (i.e. name, rating, year ofrelease) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a63b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Name  Rating  Year\n",
      "0                     Ship of Theseus     8.0  2012\n",
      "1                              Iruvar     8.4  1997\n",
      "2                     Kaagaz Ke Phool     7.8  1959\n",
      "3   Lagaan: Once Upon a Time in India     8.1  2001\n",
      "4                     Pather Panchali     8.2  1955\n",
      "..                                ...     ...   ...\n",
      "95                        Apur Sansar     8.4  1959\n",
      "96                        Kanchivaram     8.2  2008\n",
      "97                    Monsoon Wedding     7.3  2001\n",
      "98                              Black     8.1  2005\n",
      "99                            Deewaar     8.0  1975\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_imdb_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(\"Failed to fetch page.\")\n",
    "        return None\n",
    "\n",
    "import re\n",
    "def parse_imdb_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    movie_list = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "    movies_data = []\n",
    "    for movie in movie_list:\n",
    "        name = movie.find('a').text.strip()\n",
    "        rating = float(movie.find('span', class_='ipl-rating-star__rating').text.strip())\n",
    "        year_text = movie.find('span', class_='lister-item-year').text.strip()\n",
    "        # Extracting only the year from the string using regular expression\n",
    "        year_match = re.search(r'\\d{4}', year_text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group())\n",
    "            movies_data.append({'Name': name, 'Rating': rating, 'Year': year})\n",
    "\n",
    "return movies_data\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.imdb.com/list/ls056092300/'\n",
    "    html_content = fetch_imdb_page(url)\n",
    "    if html_content:\n",
    "        movies_data = parse_imdb_page(html_content)\n",
    "        df = pd.DataFrame(movies_data)\n",
    "        print(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c175f7a",
   "metadata": {},
   "source": [
    "Write a python program to scrape details of all the posts from https://www.patreon.com/coreyms .Scrape the\n",
    "heading, date, content and the likes for the video from the link for the youtube video from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cafdd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Patreon page...\n",
      "Parsing Patreon page...\n",
      "No post data found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def fetch_patreon_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(\"Failed to fetch page:\", e)\n",
    "        return None\n",
    "\n",
    "def parse_patreon_page(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        post_list = soup.find_all('div', class_='post-card')\n",
    "\n",
    "        posts_data = []\n",
    "        for post in post_list:\n",
    "            heading = post.find('h3', class_='post-card__title').text.strip()\n",
    "            date = post.find('time', class_='post-card__date')['datetime']\n",
    "            content = post.find('div', class_='post-card__content').text.strip()\n",
    "            \n",
    "            youtube_link_match = re.search(r'https://www.youtube.com/watch\\?v=[\\w-]+', content)\n",
    "            youtube_link = youtube_link_match.group() if youtube_link_match else None\n",
    "            \n",
    "            likes = None\n",
    "            if youtube_link:\n",
    "                youtube_response = requests.get(youtube_link)\n",
    "                youtube_response.raise_for_status()\n",
    "                youtube_soup = BeautifulSoup(youtube_response.text, 'html.parser')\n",
    "                likes_element = youtube_soup.find('button', class_='like-button-renderer-like-button')\n",
    "                likes = int(likes_element.find('span', class_='yt-uix-button-content').text.replace(',', ''))\n",
    "\n",
    "            posts_data.append({'Heading': heading, 'Date': date, 'Content': content, 'YouTube Link': youtube_link, 'Likes': likes})\n",
    "\n",
    "        return posts_data\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing page:\", e)\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.patreon.com/coreyms'\n",
    "    print(\"Scraping Patreon page...\")\n",
    "    html_content = fetch_patreon_page(url)\n",
    "    if html_content:\n",
    "        print(\"Parsing Patreon page...\")\n",
    "        posts_data = parse_patreon_page(html_content)\n",
    "        if posts_data:\n",
    "            for post in posts_data:\n",
    "                print(post)\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No post data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e94e247",
   "metadata": {},
   "source": [
    "Write a python program to scrape house details from mentioned URL. It should include house title, location,\n",
    "area, EMI and price from https://www.nobroker.in/ .Enter three localities which are Indira Nagar, Jayanagar,\n",
    "Rajaji Nagar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2282375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for Indira Nagar...\n",
      "Scraping data for Jayanagar...\n",
      "Scraping data for Rajaji Nagar...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(\"Failed to fetch page.\")\n",
    "        return None\n",
    "\n",
    "def parse_house_details(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    house_list = soup.find_all('div', class_='nb__2JHKO')\n",
    "\n",
    "    details_list = []\n",
    "    for house in house_list:\n",
    "        title = house.find('h2', class_='heading-6').text.strip()\n",
    "        location = house.find('div', class_='nb__2CMjv').text.strip()\n",
    "        area = house.find('div', class_='nb__3oNyC').text.strip()\n",
    "        emi = house.find('div', class_='font-semi-bold heading-6').text.strip()\n",
    "        price = house.find('div', class_='heading-7').text.strip()\n",
    "\n",
    "        details_list.append({'Title': title, 'Location': location, 'Area': area, 'EMI': emi, 'Price': price})\n",
    "\n",
    "    return details_list\n",
    "\n",
    "def main():\n",
    "    localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "    base_url = 'https://www.nobroker.in/property/sale/'\n",
    "    \n",
    "    for locality in localities:\n",
    "        url = base_url + locality.replace(' ', '-')  \n",
    "        print(f\"Scraping data for {locality}...\")\n",
    "        html_content = fetch_page_content(url)\n",
    "        if html_content:\n",
    "            house_details = parse_house_details(html_content)\n",
    "            for detail in house_details:\n",
    "                print(detail)\n",
    "                print()\n",
    "                else:\n",
    "            print(\"No data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88919c1",
   "metadata": {},
   "source": [
    "Write a python program to scrape first 10 product details which include product name , price , Image URL from\n",
    "https://www.bewakoof.com/bestseller?sort=popular ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f65f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping product details...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(\"Failed to fetch page.\")\n",
    "        return None\n",
    "\n",
    "def parse_product_details(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    product_list = soup.find_all('div', class_='productCard')\n",
    "\n",
    "    details_list = []\n",
    "    for product in product_list[:10]:  # Extract details for the first 10 products\n",
    "        name = product.find('h3', class_='product-title').text.strip()\n",
    "        price = product.find('div', class_='product-price').text.strip()\n",
    "        image_url = product.find('img', class_='product-image')['src']\n",
    "\n",
    "        details_list.append({'Name': name, 'Price': price, 'Image URL': image_url})\n",
    "\n",
    "    return details_list\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "    print(\"Scraping product details...\")\n",
    "    html_content = fetch_page_content(url)\n",
    "    if html_content:\n",
    "        product_details = parse_product_details(html_content)\n",
    "        for detail in product_details:\n",
    "            print(detail)\n",
    "            print()\n",
    "            else:\n",
    "            print(\"No data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Please visit https://www.cnbc.com/world/?region=world and scrap-\n",
    "a) headings\n",
    "\n",
    "b) date\n",
    "c) News link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d2d7089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping news details...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(\"Failed to fetch page.\")\n",
    "        return None\n",
    "\n",
    "def parse_news_details(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    news_list = soup.find_all('div', class_='Card-titleContainer')\n",
    "    date_list = soup.find_all('time', class_='Card-time')\n",
    "    link_list = soup.find_all('a', class_='Card-headline')\n",
    "\n",
    "    details_list = []\n",
    "    for heading, date, link in zip(news_list, date_list, link_list):\n",
    "        heading_text = heading.text.strip()\n",
    "        date_text = date['datetime']\n",
    "        news_link = \"https://www.cnbc.com\" + link['href']\n",
    "        details_list.append({'Heading': heading_text, 'Date': date_text, 'News Link': news_link})\n",
    "\n",
    "    return details_list\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.cnbc.com/world/?region=world'\n",
    "    print(\"Scraping news details...\")\n",
    "    html_content = fetch_page_content(url)\n",
    "    if html_content:\n",
    "        news_details = parse_news_details(html_content)\n",
    "        for detail in news_details:\n",
    "            print(detail)\n",
    "            print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4174d",
   "metadata": {},
   "source": [
    "Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-\n",
    "articles/ and scrap-\n",
    "\n",
    "a) Paper title\n",
    "b) date\n",
    "c) Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ea4c8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping paper details...\n",
      "No data found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(\"Failed to fetch page.\")\n",
    "        return None\n",
    "\n",
    "def parse_paper_details(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    paper_list = soup.find_all('div', class_='article-details')\n",
    "\n",
    "    details_list = []\n",
    "    for paper in paper_list:\n",
    "        title = paper.find('h3', class_='article-title').text.strip()\n",
    "        date = paper.find('div', class_='article-date').text.strip()\n",
    "        authors = paper.find('div', class_='article-authors').text.strip()\n",
    "\n",
    "        details_list.append({'Paper Title': title, 'Date': date, 'Authors': authors})\n",
    "\n",
    "    return details_list\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "    print(\"Scraping paper details...\")\n",
    "    html_content = fetch_page_content(url)\n",
    "    if html_content:\n",
    "        paper_details = parse_paper_details(html_content)\n",
    "        for detail in paper_details:\n",
    "            print(detail)\n",
    "            print()\n",
    "        else:\n",
    "            print(\"No data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b258e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfaa0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52405e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab2da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8d9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
