{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27bc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1&2\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_amazon_products(search_query):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.amazon.in/')\n",
    "    \n",
    "    # Enter the search query\n",
    "    search_box = driver.find_element(By.ID, 'twotabsearchtextbox')\n",
    "    search_box.send_keys(search_query)\n",
    "    search_box.submit()\n",
    "    \n",
    "    product_details = []\n",
    "    current_page = 1\n",
    "    max_pages = 3\n",
    "\n",
    "    while current_page <= max_pages:\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "        \n",
    "        for product in products:\n",
    "            try:\n",
    "                title = product.h2.text.strip()\n",
    "                url = 'https://www.amazon.in' + product.h2.a['href']\n",
    "                price = product.find('span', 'a-price-whole').text.strip()\n",
    "                try:\n",
    "                    brand = product.find('span', 'a-size-base-plus a-color-base a-text-normal').text.strip()\n",
    "                except:\n",
    "                    brand = \"-\"\n",
    "                try:\n",
    "                    availability = product.find('span', 'a-size-base a-color-price').text.strip()\n",
    "                except:\n",
    "                    availability = \"-\"\n",
    "                \n",
    "                product_details.append({\n",
    "                    'Brand Name': brand,\n",
    "                    'Name of the Product': title,\n",
    "                    'Price': price,\n",
    "                    'Return/Exchange': '-',  # Placeholder as it's not available on the search results page\n",
    "                    'Expected Delivery': '-',  # Placeholder as it's not available on the search results page\n",
    "                    'Availability': availability,\n",
    "                    'Product URL': url\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        current_page += 1\n",
    "        try:\n",
    "            next_page = driver.find_element(By.CLASS_NAME, 's-pagination-next')\n",
    "            next_page.click()\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(product_details)\n",
    "    df.to_csv(f'{search_query}_amazon_products.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Input from user\n",
    "search_query = input(\"Enter the product to search on Amazon: \")\n",
    "df = get_amazon_products(search_query)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57463dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "\n",
    "def download_google_images(keywords, num_images):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://images.google.com/')\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        search_box = driver.find_element(By.NAME, 'q')\n",
    "        search_box.clear()\n",
    "        search_box.send_keys(keyword)\n",
    "        search_box.submit()\n",
    "        \n",
    "        time.sleep(2)\n",
    "        images = driver.find_elements(By.CSS_SELECTOR, 'img.rg_i.Q4LuWd')\n",
    "        \n",
    "        if not os.path.exists(keyword):\n",
    "            os.makedirs(keyword)\n",
    "        \n",
    "        for i in range(min(num_images, len(images))):\n",
    "            try:\n",
    "                images[i].click()\n",
    "                time.sleep(2)\n",
    "                img_url = driver.find_element(By.CSS_SELECTOR, 'img.n3VNCb').get_attribute('src')\n",
    "                img_data = requests.get(img_url).content\n",
    "                with open(f'{keyword}/{keyword}_{i+1}.jpg', 'wb') as handler:\n",
    "                    handler.write(img_data)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "download_google_images(keywords, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_flipkart_smartphones(search_query):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.flipkart.com/')\n",
    "    \n",
    "    # Close the login popup\n",
    "    try:\n",
    "        close_login_popup = driver.find_element(By.XPATH, '//button[contains(text(),\"âœ•\")]')\n",
    "        close_login_popup.click()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Enter the search query\n",
    "    search_box = driver.find_element(By.NAME, 'q')\n",
    "    search_box.send_keys(search_query)\n",
    "    search_box.submit()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    smartphones = soup.find_all('div', '_1AtVbE')\n",
    "    \n",
    "    smartphone_details = []\n",
    "\n",
    "    for phone in smartphones:\n",
    "        try:\n",
    "            title = phone.find('a', 'IRpwTa').text.strip()\n",
    "            url = 'https://www.flipkart.com' + phone.find('a', '_1fQZEK')['href']\n",
    "            price = phone.find('div', '_30jeq3 _1_WHN1').text.strip()\n",
    "            \n",
    "            details = phone.find_all('li', 'rgWa7D')\n",
    "            specs = {d.text.split(\":\")[0].strip(): d.text.split(\":\")[1].strip() for d in details}\n",
    "            \n",
    "            smartphone_details.append({\n",
    "                'Brand Name': specs.get('Brand', '-'),\n",
    "                'Smartphone Name': title,\n",
    "                'Colour': specs.get('Color', '-'),\n",
    "                'RAM': specs.get('RAM', '-'),\n",
    "                'Storage(ROM)': specs.get('Internal Storage', '-'),\n",
    "                'Primary Camera': specs.get('Primary Camera', '-'),\n",
    "                'Secondary Camera': specs.get('Secondary Camera', '-'),\n",
    "                'Display Size': specs.get('Display Size', '-'),\n",
    "                'Battery Capacity': specs.get('Battery Capacity', '-'),\n",
    "                'Price': price,\n",
    "                'Product URL': url\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(smartphone_details)\n",
    "    df.to_csv(f'{search_query}_flipkart_smartphones.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Input from user\n",
    "search_query = input(\"Enter the smartphone to search on Flipkart: \")\n",
    "df = get_flipkart_smartphones(search_query)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def get_geospatial_coordinates(city_name):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://maps.google.com/')\n",
    "    \n",
    "    search_box = driver.find_element(By.ID, 'searchboxinput')\n",
    "    search_box.send_keys(city_name)\n",
    "    search_box.submit()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    url = driver.current_url\n",
    "    driver.quit()\n",
    "    \n",
    "    coords = url.split('@')[1].split(',')[0:2]\n",
    "    latitude, longitude = coords[0], coords[1]\n",
    "    \n",
    "    return latitude, longitude\n",
    "\n",
    "# Input from user\n",
    "city_name = input(\"Enter the city name to search on Google Maps: \")\n",
    "latitude, longitude = get_geospatial_coordinates(city_name)\n",
    "print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c03882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_gaming_laptops():\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.digit.in/top-products/best-gaming-laptops-40.html')\n",
    "    \n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    laptops = soup.find_all('div', 'TopNumbeHeading sticky-footer')\n",
    "    \n",
    "    laptop_details = []\n",
    "\n",
    "    for laptop in laptops:\n",
    "        try:\n",
    "            name = laptop.find('h3').text.strip()\n",
    "            specs = laptop.find_all('div', 'Spcs-details')\n",
    "            specs = {s.text.split(\":\")[0].strip(): s.text.split(\":\")[1].strip() for s in specs}\n",
    "            \n",
    "            laptop_details.append({\n",
    "                'Name': name,\n",
    "                'Processor': specs.get('Processor', '-'),\n",
    "                'Graphics': specs.get('Graphics', '-'),\n",
    "                'RAM': specs.get('Ram Included', '-'),\n",
    "                'Storage': specs.get('Storage', '-'),\n",
    "                'Price': specs.get('Price', '-')\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(laptop_details)\n",
    "    df.to_csv('gaming_laptops_digit.csv', index=False)\n",
    "    return df\n",
    "\n",
    "df = get_gaming_laptops()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def get_forbes_billionaires():\n",
    "    url = 'https://www.forbes.com/billionaires/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    billionaires = soup.find_all('div', {'class': 'TableRow_row__L-0Km'})\n",
    "    \n",
    "    billionaire_details = []\n",
    "\n",
    "    for person in billionaires:\n",
    "        try:\n",
    "            rank = person.find('div', {'class': 'Table_dataCell__mEu2b Table_dataCell__leaderboardRank__l4hx-'}).text.strip()\n",
    "            name = person.find('div', {'class': 'Table_dataCell__mEu2b Table_dataCell__name__YljcU'}).text.strip()\n",
    "            net_worth = person.find('div', {'class': 'Table_dataCell__mEu2b Table_dataCell__netWorth__bNiaH'}).text.strip()\n",
    "            age = person.find('div', {'class': 'Table_dataCell__mEu2b Table_dataCell__age__Z2vvm'}).text.strip()\n",
    "            citizenship = person.find('div', {'class': 'Table_dataCell__mEu2b Table_dataCell__countryOfCitizenship__bapTF'}).text.strip()\n",
    "            source = person.find('div', {'class': 'Table_dataCell__mEu2b Table_dataCell__source__Wvd_F'}).text.strip()\n",
    "            industry = person.find('div', {'class': 'Table_dataCell__mEu2b Table_dataCell__industry__Qq8az'}).text.strip()\n",
    "            \n",
    "            billionaire_details.append({\n",
    "                'Rank': rank,\n",
    "                'Name': name,\n",
    "                'Net worth': net_worth,\n",
    "                'Age': age,\n",
    "                'Citizenship': citizenship,\n",
    "                'Source': source,\n",
    "                'Industry': industry\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    df = pd.DataFrame(billionaire_details)\n",
    "    df.to_csv('forbes_billionaires.csv', index=False)\n",
    "    return df\n",
    "\n",
    "df = get_forbes_billionaires()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "import os\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "def get_youtube_comments(api_key, video_id, max_results=100):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            pageToken=next_page_token,\n",
    "            maxResults=100,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Upvotes': comment['likeCount'],\n",
    "                'Posted At': comment['publishedAt']\n",
    "            })\n",
    "\n",
    "            # Stop if we have enough comments\n",
    "            if len(comments) >= max_results:\n",
    "                break\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "        # Stop if there are no more pages\n",
    "        if next_page_token is None:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "def save_comments_to_csv(comments, filename='youtube_comments.csv'):\n",
    "    df = pd.DataFrame(comments)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = 'YOUR_YOUTUBE_API_KEY'  # Replace with your YouTube API key\n",
    "    video_id = 'VIDEO_ID'  # Replace with the YouTube video ID\n",
    "    max_results = 500\n",
    "\n",
    "    comments = get_youtube_comments(api_key, video_id, max_results)\n",
    "    save_comments_to_csv(comments)\n",
    "\n",
    "    print(f\"Extracted {len(comments)} comments and saved to 'youtube_comments.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ddb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_london_hostels():\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.hostelworld.com/')\n",
    "    \n",
    "    # Enter the location and search\n",
    "    search_box = driver.find_element(By.ID, 'search-input-field')\n",
    "    search_box.send_keys('London')\n",
    "    search_box.submit()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    hostels = soup.find_all('div', 'property')\n",
    "    \n",
    "    hostel_details = []\n",
    "\n",
    "    for hostel in hostels:\n",
    "        try:\n",
    "            name = hostel.find('h2', 'title').text.strip()\n",
    "            distance = hostel.find('div', 'distance').text.strip()\n",
    "            rating = hostel.find('div', 'rating-score').text.strip()\n",
    "            total_reviews = hostel.find('div', 'reviews').text.strip()\n",
    "            overall_reviews = hostel.find('div', 'keyword').text.strip()\n",
    "            privates_from = hostel.find('div', 'price').text.strip()\n",
    "            dorms_from = hostel.find('div', 'price').text.strip()\n",
    "            facilities = hostel.find('div', 'facilities').text.strip()\n",
    "            description = hostel.find('div', 'description').text.strip()\n",
    "            \n",
    "            hostel_details.append({\n",
    "                'Hostel Name': name,\n",
    "                'Distance from City Centre': distance,\n",
    "                'Ratings': rating,\n",
    "                'Total Reviews': total_reviews,\n",
    "                'Overall Reviews': overall_reviews,\n",
    "                'Privates From Price': privates_from,\n",
    "                'Dorms From Price': dorms_from,\n",
    "                'Facilities': facilities,\n",
    "                'Property Description': description\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(hostel_details)\n",
    "    df.to_csv('london_hostels.csv', index=False)\n",
    "    return df\n",
    "\n",
    "df = get_london_hostels()\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
